# Sparkify Redshift DataWarehouse Project
The objective of this project is provide an easy place to run queries over the sparkify data generated by it users, in specific all the actions when the users click **nextSong**.
In business terms, enables the data analysts to build insights based on the customers behaviour, know trending artist, trending songs and user activity. This could be use for recommenders in order to offer a personalized experience to the customers.
# About the Data Model and ETL
Please find next the data dictonary for all the tables in the final desing:

## Songplays Fact Table
| Column name | Type    | Source              | Comment                                                                                   |
|-------------|---------|---------------------|-------------------------------------------------------------------------------------------|
| songplay_id | serial  | Autogenerated       |                                                                                           |
| start_time  | bigint  | log_data.ts         | Pointing to table time.start_time                                                         |
| user_id     | int     | log_data.userId     | Pointing to table users.user_id                                                           |
| level       | varchar | log_data.level      |                                                                                           |
| song_id     | varchar | song_data.song_id   | song_id matching title, artist name of a song, DISTKEY for improve join with songs table  |
| artist_id   | varchar | song_data.artist_id | artist_id matching title, artist name of a song                                           |
| session_id  | int     | log_data.session_id |                                                                                           |
| location    | varchar | log_data.location   |                                                                                           |
| user_agent  | varchar | log_data.user_agent |                                                                                           |

## Users Dimension
Distribution Auto, let's redshift decide how to distribute this table based on the size

| Column name | Type    | Source             | Comment     |
|-------------|---------|--------------------|-------------|
| user_id     | int     | log_data.userId    | Primary key |
| first_name  | varchar | log_data.firstName |             |
| last_name   | varchar | log_data.lastName  |             |
| gender      | varchar | log_data.gender    |             |
| level       | varchar | log_data.level     |             |

### Uniqueness Policy
The process is getting the last appearence of the user building a row_number based on the last action performed (ts field) order desc getting the row_number equal to one.

## Songs Dimension
| Column name | Type    | Source              | Comment                                                  |
|-------------|---------|---------------------|----------------------------------------------------------|
| song_id     | varchar | song_data.song_id   | Primary key, DISTKEY for improve joinigs with fact table |
| title       | varchar | song_data.title     |                                                          |
| artist_id   | varchar | song_data.artist_id | Pointing to artists.artist_id                            |
| year        | int     | song_data.year      |                                                          |
| duration    | real    | song_data.duration  |                                                          |

## Artists Dimension
| Column name | Type    | Source                     | Comment     |
|-------------|---------|----------------------------|-------------|
| artist_id   | varchar | song_data.artist_id        | Primary key |
| name        | varchar | song_data.artist_name      |             |
| location    | varchar | song_data.artist_location  |             |
| latitude    | real    | song_data.artist_latitude  |             |
| longitude   | real    | song_data.artist_longitude |             |

### Uniqueness Policy
The process is getting the last appearence of the artist building a row_number based on the year field, order desc getting the row_number equal to one.

## Time Dimension
| Column name | Type      | Source      | Comment     |
|-------------|-----------|-------------|-------------|
| start_time  | timestamp | log_data.ts | Primary key |
| hour        | int       | log_data.ts |             |
| day         | int       | log_data.ts |             |
| week        | int       | log_data.ts |             |
| month       | int       | log_data.ts |             |
| year        | int       | log_data.ts |             |
| weekday     | int       | log_data.ts |             |

### Uniqueness Policy
Getting distinct values

## About files:
- sql_queries.py: Contains the scripts for create the staging and final tables, the copy to redshift queries and insert queries used in the etl.

- create_tables.py: Contains the python script to execute the postgres drop/creation actions and copy scripts.

- etl.py: Contains the complete script for process all the files.

- dwh.cfg: Configuration file, contains all the parameters used in the scripts

## How to run
Execute the creation tables script
>python create_tables.py

Execute etl script
>python etl.py

## Final Notes:
I've changed the execution order of the insert scripts in order to be able to use the songs and artists dimensions to get the corresponding ids on the fact table and avoid reprocess again the staging table to get unique songs and unique artists.